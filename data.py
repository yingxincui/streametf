import streamlit as st
import akshare as ak
import pandas as pd
import time
import os
import json
from datetime import datetime, date, timedelta
from utils import clean_etf_symbol
import hashlib

# ç¼“å­˜ç›®å½•
CACHE_DIR = "data_cache"
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

ETF_LIST_CACHE = os.path.join(CACHE_DIR, "etf_list.csv")
ETF_LIST_CACHE_TTL = 30  # å¤©

def get_cache_file_path(symbol):
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    return os.path.join(CACHE_DIR, f"{symbol}_data.csv")

def get_metadata_file_path():
    """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
    return os.path.join(CACHE_DIR, "metadata.json")

def load_metadata():
    """åŠ è½½å…ƒæ•°æ®"""
    metadata_file = get_metadata_file_path()
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            st.warning(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
    return {}

def save_metadata(metadata):
    """ä¿å­˜å…ƒæ•°æ®"""
    try:
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if not os.path.exists(CACHE_DIR):
            os.makedirs(CACHE_DIR)
            
        metadata_file = get_metadata_file_path()
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.warning(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

def is_cache_valid(symbol):
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆå½“å¤©æ•°æ®ï¼‰"""
    metadata = load_metadata()
    today = date.today().isoformat()
    # ç¡®ä¿symbolä¸ºå­—ç¬¦ä¸²ç±»å‹
    symbol_str = str(symbol)
    return metadata.get(symbol_str, {}).get('date') == today

def save_to_cache(symbol, df):
    """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
    try:
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if not os.path.exists(CACHE_DIR):
            os.makedirs(CACHE_DIR)
            
        cache_file = get_cache_file_path(symbol)
        df.to_csv(cache_file, encoding='utf-8')
        
        # æ›´æ–°å…ƒæ•°æ®
        metadata = load_metadata()
        # ç¡®ä¿symbolä¸ºå­—ç¬¦ä¸²ç±»å‹
        symbol_str = str(symbol)
        metadata[symbol_str] = {
            'date': date.today().isoformat(),
            'rows': len(df),
            'columns': list(df.columns)
        }
        save_metadata(metadata)
        
        st.success(f"{symbol}æ•°æ®å·²ç¼“å­˜")
    except Exception as e:
        st.warning(f"ç¼“å­˜{symbol}æ•°æ®å¤±è´¥: {e}")

def load_from_cache(symbol):
    """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
    try:
        cache_file = get_cache_file_path(symbol)
        if os.path.exists(cache_file):
            # ç¡®ä¿æ—¥æœŸç´¢å¼•æ­£ç¡®è§£æ
            df = pd.read_csv(cache_file, index_col=0)
            df.index = pd.to_datetime(df.index)
            return df
    except Exception as e:
        st.warning(f"åŠ è½½{symbol}ç¼“å­˜å¤±è´¥: {e}")
    return None

# è·å–æ‰€æœ‰ETFåˆ—è¡¨
@st.cache_data(ttl=24*3600)  # ç¼“å­˜24å°æ—¶ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
def get_etf_list(force_refresh=False):
    # å…ˆæŸ¥ç¼“å­˜ï¼Œé™¤éå¼ºåˆ¶åˆ·æ–°
    if not force_refresh and os.path.exists(ETF_LIST_CACHE):
        mtime = datetime.fromtimestamp(os.path.getmtime(ETF_LIST_CACHE))
        if (datetime.now() - mtime).days < ETF_LIST_CACHE_TTL:
            try:
                df = pd.read_csv(ETF_LIST_CACHE)
                if not df.empty:
                    return df
            except Exception as e:
                st.warning(f"ETFåˆ—è¡¨ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œæˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œé‡æ–°æ‹‰å–
    try:
        etf_spot = ak.fund_etf_spot_ths()
        if not etf_spot.empty:
            etf_list = etf_spot[['åŸºé‡‘ä»£ç ', 'åŸºé‡‘åç§°']].rename(columns={'åŸºé‡‘ä»£ç ': 'symbol', 'åŸºé‡‘åç§°': 'name'})
            etf_list['symbol'] = etf_list['symbol'].apply(clean_etf_symbol)
            etf_list = etf_list.drop_duplicates(subset=['symbol'])
            etf_list.to_csv(ETF_LIST_CACHE, index=False)
            st.success("ETFåˆ—è¡¨ç¼“å­˜å·²åˆ·æ–°ï¼")
            return etf_list
        else:
            st.warning("ak.fund_etf_spot_ths()è¿”å›ç©ºï¼Œå°è¯•è¯»å–æœ¬åœ°ETFåˆ—è¡¨ç¼“å­˜")
    except Exception as e:
        st.warning(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {str(e)}ï¼Œå°è¯•è¯»å–æœ¬åœ°ETFåˆ—è¡¨ç¼“å­˜")
    # å…œåº•ï¼šæœ¬åœ°csv
    if os.path.exists(ETF_LIST_CACHE):
        try:
            df = pd.read_csv(ETF_LIST_CACHE)
            return df
        except Exception as e:
            st.error(f"æœ¬åœ°ETFåˆ—è¡¨ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    return pd.DataFrame(columns=['symbol', 'name'])

# è·å–ETFå†å²æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶+CSVç¼“å­˜ï¼‰
def fetch_etf_data_with_retry(symbol, start_date, end_date, etf_list, max_retries=3, delay=1):
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if is_cache_valid(symbol):
        cached_data = load_from_cache(symbol)
        if cached_data is not None:
            # ç­›é€‰æ—¶é—´èŒƒå›´
            start_date = pd.to_datetime(start_date)
            end_date = pd.to_datetime(end_date)
            filtered_data = cached_data[(cached_data.index >= start_date) & (cached_data.index <= end_date)]
            if not filtered_data.empty:
                st.info(f"ä½¿ç”¨{symbol}ç¼“å­˜æ•°æ®")
                # è¿”å›ç¬¦åˆç»„åˆå›æµ‹æ ¼å¼çš„æ•°æ®
                clean_symbol = clean_etf_symbol(symbol)
                row = etf_list[etf_list['symbol'] == symbol]
                if not row.empty:
                    etf_name = row['name'].values[0]
                else:
                    etf_name = symbol
                    st.warning(f"ETFä»£ç  {symbol} ä¸åœ¨ETFåˆ—è¡¨ä¸­ï¼Œå·²ç”¨ä»£ç ä½œä¸ºåç§°")
                return filtered_data[['æ”¶ç›˜']].rename(columns={'æ”¶ç›˜': f"{clean_symbol}_{etf_name}"})
    
    # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œä»APIè·å–å®Œæ•´æ•°æ®
    st.info(f"ä»APIè·å–{symbol}å®Œæ•´æ•°æ®...")
    for attempt in range(max_retries):
        try:
            clean_symbol = clean_etf_symbol(symbol)
            row = etf_list[etf_list['symbol'] == symbol]
            if not row.empty:
                etf_name = row['name'].values[0]
            else:
                etf_name = symbol
                st.warning(f"ETFä»£ç  {symbol} ä¸åœ¨ETFåˆ—è¡¨ä¸­ï¼Œå·²ç”¨ä»£ç ä½œä¸ºåç§°")
            st.write(f"ğŸ“¥ è·å– {etf_name}({symbol}) çš„å®Œæ•´å†å²æ•°æ®...")
            
            # è·å–å®Œæ•´æ•°æ®ï¼ˆä¸æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰
            df = ak.fund_etf_hist_em(
                symbol=clean_symbol,
                period='daily',
                adjust='qfq'
            )
            
            if df.empty:
                st.warning(f"âš ï¸ {etf_name}({symbol}) è¿”å›ç©ºæ•°æ®")
                return pd.DataFrame()
            
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            df.set_index('æ—¥æœŸ', inplace=True)
            
            # ä¿å­˜å®Œæ•´æ•°æ®åˆ°ç¼“å­˜
            save_to_cache(symbol, df)
            
            # ç­›é€‰è¯·æ±‚çš„æ—¶é—´èŒƒå›´
            start_date = pd.to_datetime(start_date)
            end_date = pd.to_datetime(end_date)
            filtered_df = df[(df.index >= start_date) & (df.index <= end_date)]
            
            if filtered_df.empty:
                st.warning(f"âš ï¸ {etf_name}({symbol}) åœ¨é€‰å®šæ—¥æœŸèŒƒå›´å†…æ— æ•°æ®")
                return pd.DataFrame()
                
            date_range = f"{filtered_df.index.min().strftime('%Y-%m-%d')} ~ {filtered_df.index.max().strftime('%Y-%m-%d')}"
            days = len(filtered_df)
            st.write(f"ğŸ“† {etf_name}({symbol}) æ•°æ®åŒºé—´ï¼š{date_range}ï¼Œå…± {days} å¤©")
            
            # è¿”å›ç¬¦åˆç»„åˆå›æµ‹æ ¼å¼çš„æ•°æ®
            return filtered_df[['æ”¶ç›˜']].rename(columns={'æ”¶ç›˜': f"{clean_symbol}_{etf_name}"})
            
        except Exception as e:
            if attempt < max_retries - 1:
                st.warning(f"âš ï¸ è·å– {symbol} æ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}), å°†åœ¨ {delay}ç§’åé‡è¯•...")
                time.sleep(delay)
                continue
            st.warning(f"âš ï¸ æ— æ³•è·å– {symbol} çš„æ•°æ®: {str(e)}")
            # å°è¯•ä½¿ç”¨å†å²ç¼“å­˜æ•°æ®
            cached_data = load_from_cache(symbol)
            if cached_data is not None:
                start_date = pd.to_datetime(start_date)
                end_date = pd.to_datetime(end_date)
                filtered_data = cached_data[(cached_data.index >= start_date) & (cached_data.index <= end_date)]
                if not filtered_data.empty:
                    st.warning(f"ä½¿ç”¨{symbol}å†å²ç¼“å­˜æ•°æ®")
                    clean_symbol = clean_etf_symbol(symbol)
                    row = etf_list[etf_list['symbol'] == symbol]
                    if not row.empty:
                        etf_name = row['name'].values[0]
                    else:
                        etf_name = symbol
                        st.warning(f"ETFä»£ç  {symbol} ä¸åœ¨ETFåˆ—è¡¨ä¸­ï¼Œå·²ç”¨ä»£ç ä½œä¸ºåç§°")
                    return filtered_data[['æ”¶ç›˜']].rename(columns={'æ”¶ç›˜': f"{clean_symbol}_{etf_name}"})
            return pd.DataFrame()

if st.button("è®©AIåˆ†æåˆé€‚çš„æŠ•èµ„ç­–ç•¥"):
    st.info("å·²å‘é€AIè¯·æ±‚ï¼Œæ­£åœ¨ç­‰å¾…AIè¿”å›ç»“æœ...")
    try:
        csv_data = df.head(200).to_csv(index=False)
        prompt = f"ä»¥ä¸‹æ˜¯æŸäº›èµ„äº§çš„å†å²æ—¥æœŸå’Œæ”¶ç›˜ä»·æ•°æ®ï¼Œè¯·ä½ ä½œä¸ºä¸“ä¸šæŠ•èµ„é¡¾é—®ï¼Œåˆ†æé€‚åˆçš„æŠ•èµ„ç­–ç•¥ï¼ˆå¦‚å®šæŠ•ã€è½®åŠ¨ã€è¶‹åŠ¿ã€å‡å€¼å›å½’ç­‰ï¼‰ï¼Œå¹¶è¯´æ˜ç†ç”±å’Œæ³¨æ„äº‹é¡¹ï¼š\n{csv_data}"
        with st.spinner("AIæ­£åœ¨åˆ†æï¼Œè¯·ç¨å€™..."):
            result = ai_chat(prompt, api_key=api_key)
        if not result or result.strip() == "":
            st.error("AIæœªè¿”å›ç»“æœï¼Œè¯·æ£€æŸ¥API Keyå’Œç½‘ç»œï¼Œæˆ–ç¨åé‡è¯•ã€‚")
        elif "AIè°ƒç”¨å¤±è´¥" in result:
            st.error(result)
        else:
            st.markdown("#### AIç­–ç•¥å»ºè®®ï¼š")
            st.write(result)
    except Exception as e:
        st.error(f"AIåˆ†æå¼‚å¸¸: {e}")